{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8108350e",
   "metadata": {},
   "source": [
    "### Accuracy Analysis of the Created Classification Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67b9e5",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee45348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, cohen_kappa_score, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6a282",
   "metadata": {},
   "source": [
    "##### 1) Comparing classification results with assigned classes for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb365c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['USE_PYGEOS'] = '0'\n",
    "shp_folders = ['']\n",
    "output_folder = ''\n",
    "tif_folder = ''\n",
    "\n",
    "tif_files = [file for file in os.listdir(tif_folder) if file.endswith('.tif')]\n",
    "\n",
    "start = time.process_time()\n",
    "\n",
    "for shp_folder in shp_folders:\n",
    "    for shp_file_name in os.listdir(shp_folder):\n",
    "        if shp_file_name.endswith('.shp'):\n",
    "            shp_file_path = os.path.join(shp_folder, shp_file_name)\n",
    "            base_name_without_extension = os.path.splitext(shp_file_name)[0]\n",
    "            shp_crs = gpd.read_file(shp_file_path).crs  \n",
    "            for tif_file_name in tif_files:\n",
    "                tif_file_to_process = os.path.join(tif_folder, tif_file_name)\n",
    "                with rasterio.open(tif_file_to_process) as tif_src:\n",
    "                    tif_crs = tif_src.crs\n",
    "\n",
    "                pointData = gpd.read_file(shp_file_path)\n",
    "                pointData = pointData[pointData['geometry'].notnull()]\n",
    "                pointData['geometry'] = pointData['geometry'].apply(lambda geom: geom if geom.type == 'Point' else geom.centroid)\n",
    "\n",
    "                def get_raster_values(tif_file, coords):\n",
    "                    with rasterio.open(tif_file) as src:\n",
    "                        values = [int(x[0]) for x in src.sample(coords)] \n",
    "                    return values\n",
    "                column_name = os.path.splitext(os.path.basename(tif_file_to_process))[0]\n",
    "                coord_list = [(x, y) for x, y in zip(pointData[\"geometry\"].x, pointData[\"geometry\"].y)]\n",
    "                pointData[column_name] = get_raster_values(tif_file_to_process, coord_list)\n",
    "                result_df = pd.DataFrame({\n",
    "                    'geometry': pointData['geometry'],\n",
    "                    'target': pointData['class'],\n",
    "                    'predicted': pointData[column_name]\n",
    "                })\n",
    "                output_file_name = f'{base_name_without_extension}_predicted.csv'\n",
    "                output_csv_path = os.path.join(output_folder, output_file_name)\n",
    "                result_df.to_csv(output_csv_path, index=False)\n",
    "                print(f\"\\nCRS of {shp_file_name}: {shp_crs}\")\n",
    "                print(f\"CRS of {tif_file_name}: {tif_crs}\")\n",
    "                print(f\"\\nFor the number of training samples in {shp_file_name}:\", result_df.shape[0])\n",
    "                print(f\"Saved result to {output_csv_path}\")\n",
    "                print(result_df.head(6000))\n",
    "\n",
    "print(\"\\nProcessing time is [s]\", time.process_time() - start)\n",
    "print(\"\\nAll results saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a23432",
   "metadata": {},
   "source": [
    "##### 2) Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55736d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = ''\n",
    "output_file_path = ''  \n",
    "confusion_matrix_folder = ''\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith('.csv')]\n",
    "confusion_matrices = []\n",
    "start = time.process_time()\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for csv_file_name in csv_files:\n",
    "        csv_file_path = os.path.join(csv_folder, csv_file_name)\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df['target'] = pd.to_numeric(df['target'], errors='coerce')\n",
    "        df['predicted'] = pd.to_numeric(df['predicted'], errors='coerce')\n",
    "        df_filtered = df[(df['target'].between(1, 6)) & (df['predicted'].between(1, 6))]\n",
    "        f1_all_classes = \"{:.4f}\".format(f1_score(df_filtered['target'], df_filtered['predicted'], average='weighted'))\n",
    "        f1_scores_per_class = {}\n",
    "        for class_label in range(1, 7):  \n",
    "            f1_score_class = \"{:.4f}\".format(f1_score(df_filtered['target'], df_filtered['predicted'], labels=[class_label], average=None)[0])\n",
    "            f1_scores_per_class[class_label] = f1_score_class\n",
    "        output_file.write(\"\\nF1-scores for each class:\\n\")\n",
    "        for class_label, score in f1_scores_per_class.items():\n",
    "            output_file.write(f\"Class {class_label}: {score}\\n\")         \n",
    "        f1_all_classes = \"{:.4f}\".format(f1_score(df_filtered['target'], df_filtered['predicted'], average='weighted'))\n",
    "        f1_class_6 = \"{:.4f}\".format(f1_score(df_filtered['target'], df_filtered['predicted'], labels=[6], average=None)[0])\n",
    "\n",
    "        overall_accuracy_all_classes = accuracy_score(df_filtered['target'], df_filtered['predicted'])\n",
    "        kappa_score_all_classes = cohen_kappa_score(df_filtered['target'], df_filtered['predicted'])\n",
    "        class_6_indices = df_filtered['target'] == 6\n",
    "        overall_accuracy_class_6 = accuracy_score(df_filtered[class_6_indices]['target'], df_filtered[class_6_indices]['predicted'])\n",
    "        kappa_score_class_6 = cohen_kappa_score(df_filtered[class_6_indices]['target'], df_filtered[class_6_indices]['predicted'])\n",
    "\n",
    "        overall_accuracy = \"{:.4f}\".format(overall_accuracy_all_classes)\n",
    "        kappa_score = \"{:.4f}\".format(kappa_score_all_classes)\n",
    "        auc_per_class = {}\n",
    "        roc_per_class = {}\n",
    "        for class_label in range(1, 7): \n",
    "            binary_labels = df_filtered['target'] == class_label\n",
    "            fpr, tpr, thresholds = roc_curve(binary_labels, (df_filtered['predicted'] == class_label).astype(int))\n",
    "            auc_value = \"{:.4f}\".format(auc(fpr, tpr))\n",
    "            auc_per_class[f' class {class_label}'] = auc_value\n",
    "            roc_per_class[class_label] = (fpr, tpr, thresholds)\n",
    "        user_accuracy_per_class = {}\n",
    "        producer_accuracy_per_class = {}\n",
    "        for class_label in range(1, 7):\n",
    "            binary_target = (df_filtered['target'] == class_label).astype(int)\n",
    "            binary_predicted = (df_filtered['predicted'] == class_label).astype(int)\n",
    "            user_accuracy = \"{:.4f}\".format(precision_score(binary_target, binary_predicted))\n",
    "            producer_accuracy = \"{:.4f}\".format(recall_score(binary_target, binary_predicted))\n",
    "            user_accuracy_per_class[f'User_Accuracy_class_{class_label}'] = user_accuracy\n",
    "            producer_accuracy_per_class[f'Producer_Accuracy_class_{class_label}'] = producer_accuracy\n",
    "        auc_values = [float(auc_value) for auc_value in auc_per_class.values()]\n",
    "        auc_average = \"{:.4f}\".format(sum(auc_values) / len(auc_values))\n",
    "\n",
    "        output_file.write(\"_\" * 70)\n",
    "        output_file.write(f\"\\nResult for {csv_file_name}:\\n\")\n",
    "\n",
    "        output_file.write(\"\\nF1-scores for each class:\\n\")\n",
    "        for class_label, score in f1_scores_per_class.items():\n",
    "            output_file.write(f\"Class {class_label}: {score}\\n\")\n",
    "            \n",
    "        output_file.write(f\"\\nF1-score for all classes: {f1_all_classes}\\n\")\n",
    "        output_file.write(f\"F1-score for class 6: {f1_class_6}\\n\")\n",
    "        \n",
    "        output_file.write(f\"\\nCohen's Kappa: {kappa_score}\\n\")\n",
    "        output_file.write(f\"Overall Accuracy: {overall_accuracy}\\n\")\n",
    "        \n",
    "        output_file.write(f\"\\nAUC average for all classes: {auc_average}\\n\")\n",
    "        for class_label, auc_value in auc_per_class.items():\n",
    "            output_file.write(f\"AUC for{class_label}: {auc_value}\\n\")\n",
    "            \n",
    "        class_report = classification_report(\n",
    "            df_filtered['target'],\n",
    "            df_filtered['predicted'],\n",
    "            digits=4,\n",
    "        )\n",
    "\n",
    "        output_file.write(f\"\\nClassification Report:\\n\")\n",
    "        output_file.write(class_report + '\\n')s\n",
    "        for class_label in range(1, 7):\n",
    "            output_file.write(f\"\\nUser Accuracy for class {class_label}: {user_accuracy_per_class[f'User_Accuracy_class_{class_label}']}\\n\")\n",
    "            output_file.write(f\"Producer Accuracy for class {class_label}: {producer_accuracy_per_class[f'Producer_Accuracy_class_{class_label}']}\\n\")\n",
    "        conf_matrix = confusion_matrix(df_filtered['target'], df_filtered['predicted'])\n",
    "        TP = conf_matrix[1, 1]\n",
    "        FP = conf_matrix[0, 1]\n",
    "        TN = conf_matrix[0, 0]\n",
    "        FN = conf_matrix[1, 0]\n",
    "        TPR = \"{:.4f}\".format(TP / (TP + FN) * 100)\n",
    "        FPR = \"{:.4f}\".format(FP / (FP + TN) * 100)\n",
    "        TNR = \"{:.4f}\".format(TN / (TN + FP) * 100) \n",
    "        FNR = \"{:.4f}\".format(FN / (TP + FN) * 100) \n",
    "        \n",
    "        output_file.write(f\"\\nTrue Positive Rate (TPR): {TPR}%\\n\")\n",
    "        output_file.write(f\"False Positive Rate (FPR): {FPR}%\\n\")\n",
    "        output_file.write(f\"True Negative Rate (TNR): {TNR}%\\n\")\n",
    "        output_file.write(f\"False Negative Rate (FNR): {FNR}%\\n\")\n",
    "        \n",
    "        for class_label, roc_values in roc_per_class.items():\n",
    "            fpr, tpr, _ = roc_values\n",
    "            fpr_percent = [val * 100 for val in fpr]\n",
    "            tpr_percent = [val * 100 for val in tpr]\n",
    "            \n",
    "            plt.figure(figsize=(7/ 2.54, 7 / 2.54))\n",
    "            plt.plot(fpr_percent, tpr_percent, color='darkorange', lw=2, label='Krzywa ROC')\n",
    "            plt.plot([0, 100], [0, 100], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 100.0])\n",
    "            plt.ylim([0.0, 100.0])\n",
    "            plt.xlabel('FPR [%]', fontsize=10)\n",
    "            plt.ylabel('TPR [%]', fontsize=10)\n",
    "            plt.title(f'Krzywa ROC dla klasy {class_label}', fontsize=12)\n",
    "            plt.legend(loc='lower right')\n",
    "            roc_curve_filename = f'{class_label}_{csv_file_name}_roc_curve.png'\n",
    "            roc_curve_filepath = os.path.join(confusion_matrix_folder, roc_curve_filename)\n",
    "            plt.savefig(roc_curve_filepath, bbox_inches='tight', pad_inches=0.1, dpi=400)\n",
    "            plt.tight_layout()\n",
    "            plt.close()\n",
    "\n",
    "        conf_matrix = confusion_matrix(df_filtered['target'], df_filtered['predicted'])\n",
    "        confusion_matrix_filename = os.path.splitext(csv_file_name)[0] + '_confusion_matrix.png'\n",
    "        confusion_matrix_filepath = os.path.join(confusion_matrix_folder, confusion_matrix_filename)\n",
    "        labels = ['1', '2', '3', '4', '5', '6']\n",
    "        plt.figure(figsize=(10/2.54, 8/2.54))\n",
    "        ax = sns.heatmap(conf_matrix * 100 / conf_matrix.sum(axis=1)[:, None], annot=True, cmap=\"Greens\", fmt='.2f', cbar=True, xticklabels=labels, yticklabels=labels)\n",
    "        confusion_matrix_title = os.path.splitext(csv_file_name)[0][:-10]\n",
    "        ax.set_xlabel(\"Przewidywane\", fontsize=9)\n",
    "        ax.set_ylabel(\"Rzeczywiste\", fontsize=9)\n",
    "        plt.title(f\"Macierz Pomy≈Çek [%]\", fontsize=9)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "        for text in ax.texts:\n",
    "            text.set_fontsize(8)\n",
    "        plt.tight_layout(pad=1.5)\n",
    "        plt.savefig(confusion_matrix_filepath, bbox_inches='tight', pad_inches=0.1, dpi=800)\n",
    "        plt.close()\n",
    "        confusion_matrices.append(conf_matrix)\n",
    "        \n",
    "print(\"Processing time in [s]\", time.process_time() - start)\n",
    "print(\"\\nAll results saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
